"""
ì „ë¬¸ê°€ ì–´ë“œë°”ì´ìŠ¤ íˆ´: ì§ë¬´ë³„ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ì œê³µ

MCP ìŠ¤íƒ€ì¼ ë™ì  í”„ë¡¬í”„íŠ¸ ë¡œë”© ì‹œìŠ¤í…œ
"""

from pathlib import Path

import streamlit as st
import yaml
from langchain.tools import ToolRuntime, tool
from pydantic import BaseModel, Field

from schemas import JobRole


class PromptMetadata(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ë©”íƒ€ë°ì´í„° (YAML frontmatter)"""

    name: str = Field(description="í”„ë¡¬í”„íŠ¸ ì´ë¦„")
    description: str = Field(description="í”„ë¡¬í”„íŠ¸ ì„¤ëª…")
    category: str = Field(description="ì¹´í…Œê³ ë¦¬ (engineering, analysis, etc.)")


class ExpertPrompt(BaseModel):
    """ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ì „ì²´ êµ¬ì¡°"""

    metadata: PromptMetadata
    content: str = Field(description="í”„ë¡¬í”„íŠ¸ ë³¸ë¬¸ (Markdown)")
    file_path: str = Field(description="ì›ë³¸ íŒŒì¼ ê²½ë¡œ")


class PromptLoader:
    """
    MCP ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ë™ì  ë¡œë”

    Features:
    - ìë™ ë°œê²¬: prompts/ ë””ë ‰í† ë¦¬ ìŠ¤ìº”
    - Lazy Loading: í•„ìš”í•œ í”„ë¡¬í”„íŠ¸ë§Œ ë¡œë“œ
    - ìºì‹±: ë‚´ë¶€ ë”•ì…”ë„ˆë¦¬ë¡œ ì¤‘ë³µ ë¡œë“œ ë°©ì§€
    - Fallback: ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    - Streamlit í†µí•©: @st.cache_resourceë¡œ ì•± ì „ì²´ ê³µìœ 
    """

    # JobRole â†’ Prompt íŒŒì¼ëª… ë§¤í•‘
    ROLE_TO_PROMPT_MAP = {
        JobRole.BACKEND: "backend-architect",
        JobRole.FRONTEND: "frontend-architect",
        JobRole.DEVOPS: "devops-architect",
        JobRole.DATA_ENGINEER: "python-expert",
        JobRole.ML_ENGINEER: "python-expert",
        JobRole.FULLSTACK: "system-architect",  # í’€ìŠ¤íƒì€ ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸
        JobRole.IOS: "frontend-architect",  # iOSë„ í”„ë¡ íŠ¸ì—”ë“œ ë²”ì£¼
        JobRole.ANDROID: "frontend-architect",  # AOSë„ í”„ë¡ íŠ¸ì—”ë“œ ë²”ì£¼
        JobRole.ETC: "learning-guide",  # ê¸°íƒ€ëŠ” í•™ìŠµ ê°€ì´ë“œ
    }

    def __init__(self, prompts_dir: str | None = None):
        """
        Args:
            prompts_dir: í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸: í”„ë¡œì íŠ¸ ë£¨íŠ¸/prompts)
        """
        if prompts_dir is None:
            # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent
            prompts_dir = project_root / "prompts"  # type: ignore

        self.prompts_dir = Path(prompts_dir)  # type: ignore
        self._cache: dict[str, ExpertPrompt] = {}  # ë‚´ë¶€ ìºì‹œ ë”•ì…”ë„ˆë¦¬

        if not self.prompts_dir.exists():
            raise FileNotFoundError(f"Prompts directory not found: {self.prompts_dir}")

    def load_prompt(self, prompt_name: str) -> ExpertPrompt | None:
        """
        ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ìºì‹±ë¨)

        Args:
            prompt_name: í”„ë¡¬í”„íŠ¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)

        Returns:
            ExpertPrompt ë˜ëŠ” None (íŒŒì¼ ì—†ìŒ)
        """
        # ìºì‹œ í™•ì¸
        if prompt_name in self._cache:
            return self._cache[prompt_name]

        file_path = self.prompts_dir / f"{prompt_name}.md"

        if not file_path.exists():
            return None

        # íŒŒì¼ ì½ê¸°
        with open(file_path, encoding="utf-8") as f:
            content = f.read()

        # YAML frontmatter + Markdown ë¶„ë¦¬
        if content.startswith("---"):
            # frontmatter íŒŒì‹±
            parts = content.split("---", 2)
            if len(parts) >= 3:
                frontmatter_str = parts[1].strip()
                markdown_content = parts[2].strip()

                # YAML íŒŒì‹±
                try:
                    frontmatter_data = yaml.safe_load(frontmatter_str)
                    metadata = PromptMetadata(**frontmatter_data)
                except (yaml.YAMLError, ValueError) as e:
                    print(f"âš ï¸ YAML íŒŒì‹± ì‹¤íŒ¨: {file_path.name} - {e}")
                    return None

                prompt = ExpertPrompt(
                    metadata=metadata,
                    content=markdown_content,
                    file_path=str(file_path),
                )

                # ìºì‹œì— ì €ì¥
                self._cache[prompt_name] = prompt
                return prompt

        # frontmatter ì—†ëŠ” ê²½ìš°
        print(f"âš ï¸ Frontmatter ì—†ìŒ: {file_path.name}")
        return None

    def scan_all_prompts(self) -> dict[str, ExpertPrompt]:
        """
        ëª¨ë“  í”„ë¡¬í”„íŠ¸ ìŠ¤ìº” ë° ë¡œë“œ

        Returns:
            {prompt_name: ExpertPrompt} ë”•ì…”ë„ˆë¦¬
        """
        prompts = {}

        for md_file in self.prompts_dir.glob("*.md"):
            prompt_name = md_file.stem
            prompt = self.load_prompt(prompt_name)

            if prompt:
                prompts[prompt_name] = prompt

        return prompts

    def get_by_role(self, job_role: JobRole) -> ExpertPrompt | None:
        """
        JobRoleì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ë°˜í™˜

        Args:
            job_role: ì§ë¬´ ì—­í•  (Enum)

        Returns:
            í•´ë‹¹ ì—­í• ì˜ ExpertPrompt ë˜ëŠ” None
        """
        # ë§¤í•‘ í…Œì´ë¸”ì—ì„œ í”„ë¡¬í”„íŠ¸ëª… ì°¾ê¸°
        prompt_name = self.ROLE_TO_PROMPT_MAP.get(job_role)

        if not prompt_name:
            print(f"âš ï¸ JobRole ë§¤í•‘ ì—†ìŒ: {job_role}")
            return None

        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ìºì‹±ë¨)
        return self.load_prompt(prompt_name)

    def get_fallback_prompt(self) -> str:
        """
        ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸

        Returns:
            ë²”ìš© ì–´ë“œë°”ì´ìŠ¤ í”„ë¡¬í”„íŠ¸
        """
        return """
## ì¤‘ë‹ˆì–´ ê°œë°œìë¥¼ ìœ„í•œ ì»¤ë¦¬ì–´ ì¡°ì–¸

ë‹¹ì‹ ì€ ì¤‘ê¸‰ ê°œë°œì(ì¤‘ë‹ˆì–´)ì˜ ì»¤ë¦¬ì–´ ì„±ì¥ì„ ë•ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ì ‘ê·¼ ë°©ì‹:
1. **ê¸°ìˆ  ì—­ëŸ‰ í‰ê°€**: í˜„ì¬ ê¸°ìˆ  ìŠ¤íƒê³¼ ê²½í—˜ ì—°ì°¨ ë¶„ì„
2. **ì„±ì¥ ë°©í–¥ ì œì‹œ**: ê°œì¸ ìƒí™©ì— ë§ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì„±ì¥ ë¡œë“œë§µ
3. **ì‹¤ë¬´ ì¸ì‚¬ì´íŠ¸**: ì‹¤ì œ í˜„ì¥ ê²½í—˜ê³¼ ì‚¬ë¡€ ê¸°ë°˜ ì¡°ì–¸
4. **ì¥ê¸° ê´€ì **: ë‹¨ê¸° í•´ê²°ì±…ì´ ì•„ë‹Œ ì§€ì† ê°€ëŠ¥í•œ ì»¤ë¦¬ì–´ ë°œì „ ë°©í–¥

### ì¡°ì–¸ ì›ì¹™:
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ ì œì‹œ
- ê°œì¸ì˜ ìƒí™©ê³¼ ê´€ì‹¬ì‚¬ ê³ ë ¤
- ì—…ê³„ íŠ¸ë Œë“œì™€ ì‹¤ë¬´ ê²½í—˜ ë°˜ì˜
- ê¸ì •ì ì´ë©´ì„œë„ í˜„ì‹¤ì ì¸ í”¼ë“œë°±
"""


@st.cache_resource(show_spinner="ğŸ”„ ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ ë¡œë” ì´ˆê¸°í™” ì¤‘...", ttl=3600)
def get_prompt_loader() -> PromptLoader:
    """
    PromptLoader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (Streamlit ìºì‹±)

    Streamlit cache_resourceë¡œ ì•± ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ìƒì„±
    TTL: 1ì‹œê°„ (3600ì´ˆ)

    Returns:
        PromptLoader ì¸ìŠ¤í„´ìŠ¤
    """
    return PromptLoader()


# ========================================
# LangChain Tool ì •ì˜
# ========================================


class ExpertSearchInput(BaseModel):
    """ì „ë¬¸ê°€ ê²€ìƒ‰ ì…ë ¥ ìŠ¤í‚¤ë§ˆ"""

    job_role: str = Field(description="ì§ë¬´ (ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, DevOps, iOS, AOS, í’€ìŠ¤íƒ, ë°ì´í„° ì—”ì§€ë‹ˆì–´, ML ì—”ì§€ë‹ˆì–´, ê¸°íƒ€)")


@tool("expert", args_schema=ExpertSearchInput)
def expert_search(job_role: str, runtime: ToolRuntime | None = None) -> str:
    """
    ì§ë¬´ì— ë§ëŠ” ì „ë¬¸ê°€ ì–´ë“œë°”ì´ìŠ¤ í”„ë¡¬í”„íŠ¸ ê²€ìƒ‰ ë„êµ¬

    ì‚¬ìš©ìì˜ ì§ë¬´ì— ë”°ë¼ íŠ¹í™”ëœ ì»¤ë¦¬ì–´ ì¡°ì–¸ê³¼ ì„±ì¥ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    Args:
        job_role: ì‚¬ìš©ìì˜ ì§ë¬´ (ì˜ˆ: "ë°±ì—”ë“œ", "í”„ë¡ íŠ¸ì—”ë“œ", "DevOps")
        runtime: LangGraph ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ (optional)

    Returns:
        í•´ë‹¹ ì§ë¬´ì— ë§ëŠ” ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
    """
    # Stream writer ì´ˆê¸°í™”
    writer = runtime.stream_writer if runtime else None
    if writer:
        writer(f"ğŸ” ì „ë¬¸ê°€ ì–´ë“œë°”ì´ìŠ¤ ê²€ìƒ‰ ì¤‘: {job_role}")

    # PromptLoader ì¸ìŠ¤í„´ìŠ¤
    loader = get_prompt_loader()

    # job_role ë¬¸ìì—´ â†’ JobRole Enum ë³€í™˜
    try:
        # "ë°±ì—”ë“œ" â†’ JobRole.BACKEND
        role_enum = None
        for role in JobRole:
            if role.value == job_role:
                role_enum = role
                break

        if not role_enum:
            if writer:
                writer(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§ë¬´: {job_role}")
            return loader.get_fallback_prompt()

    except ValueError:
        if writer:
            writer(f"âš ï¸ ì§ë¬´ íŒŒì‹± ì‹¤íŒ¨: {job_role}")
        return loader.get_fallback_prompt()

    # JobRoleì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    expert_prompt = loader.get_by_role(role_enum)

    if not expert_prompt:
        if writer:
            writer("âš ï¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨, Fallback ì‚¬ìš©")
        return loader.get_fallback_prompt()

    # ì„±ê³µ
    if writer:
        writer(f"âœ… í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ: {expert_prompt.metadata.name}")

    # ë©”íƒ€ë°ì´í„° + ë³¸ë¬¸ ê²°í•©
    result = f"""# {expert_prompt.metadata.name}

**ì„¤ëª…**: {expert_prompt.metadata.description}
**ì¹´í…Œê³ ë¦¬**: {expert_prompt.metadata.category}

---

{expert_prompt.content}
"""

    return result


# ========================================
# CLI í…ŒìŠ¤íŠ¸ìš© (ì„ íƒì‚¬í•­)
# ========================================

if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰:
    python -m tools.expert_advice
    """
    print("=" * 80)
    print("ğŸ”§ Expert Advice Prompt Loader Test")
    print("=" * 80)

    loader = get_prompt_loader()

    # 1. ëª¨ë“  í”„ë¡¬í”„íŠ¸ ìŠ¤ìº”
    print("\nğŸ“‚ Scanning all prompts...")
    all_prompts = loader.scan_all_prompts()
    print(f"âœ… Found {len(all_prompts)} prompts:")
    for name, prompt in all_prompts.items():
        print(f"  - {name}: {prompt.metadata.description}")

    # 2. JobRoleë³„ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ Testing JobRole mappings...")
    test_roles = [JobRole.BACKEND, JobRole.FRONTEND, JobRole.DEVOPS, JobRole.DATA_ENGINEER]

    for role in test_roles:
        prompt = loader.get_by_role(role)
        if prompt:
            print(f"  âœ… {role.value}: {prompt.metadata.name}")
        else:
            print(f"  âŒ {role.value}: Not found")

    # 3. Tool ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ”¨ Testing LangChain tool...")
    result = expert_search.invoke({"job_role": "ë°±ì—”ë“œ"})
    print(f"âœ… Tool returned {len(result)} characters")
    print(f"\nFirst 300 chars:\n{result[:300]}...")
