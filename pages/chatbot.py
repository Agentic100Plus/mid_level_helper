"""
Streamlit ì±„íŒ… UI

ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ ReAct Agent ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
- ì‹¤ì‹œê°„ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° (LangChain stream_mode="messages")
- ì¤‘ê°„ ê³¼ì • ë¡œê·¸ í‘œì‹œ (Tool calls, Results)
- ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
"""

import streamlit as st
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

from main import get_gemini
from middleware.middleware import common_middlewares, dynamic_system_prompt
from schemas import UserProfile
from tools import ddgs_search, expert_search, sementic_search

# íˆ´ ë“±ë¡
tools = [
    ddgs_search,
    sementic_search,
    expert_search,
]
# ========================================
# Page Configuration
# ========================================

st.set_page_config(
    page_title="ì¤‘ë‹ˆì–´ ê³ ë¯¼ ìƒë‹´ ì±—ë´‡",
    page_icon="ğŸ’¬",
    layout="wide",
)

st.title("ğŸ’¬ ì¤‘ë‹ˆì–´ ê³ ë¯¼ ìƒë‹´ ì±—ë´‡")
st.caption("AIê°€ ë‹¹ì‹ ì˜ ê³ ë¯¼ì„ í•¨ê»˜ í•´ê²°í•©ë‹ˆë‹¤")

# ========================================
# Session State ì´ˆê¸°í™”
# ========================================

if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

if "user_profile" not in st.session_state:
    st.error("âŒ ì‚¬ìš©ì í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì—ì„œ í”„ë¡œí•„ì„ ë“±ë¡í•˜ì„¸ìš”.")
    st.stop()

# ========================================
# Sidebar: í”„ë¡œí•„ ì •ë³´
# ========================================

with st.sidebar:
    st.header("ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„")

    profile: UserProfile = st.session_state.user_profile

    st.write(f"**ì´ë¦„**: {profile.name}")
    st.write(f"**ê²½ë ¥**: {profile.career_level.value} ({profile.years_of_experience}ë…„ì°¨)")
    st.write(f"**ì§ë¬´**: {profile.job_role.value}")
    st.write(f"**ê¸°ìˆ ìŠ¤íƒ**: {', '.join(profile.tech_stack)}")

    if profile.company_size:
        st.write(f"**íšŒì‚¬ê·œëª¨**: {profile.company_size}")
    if profile.work_style:
        st.write(f"**ê·¼ë¬´í˜•íƒœ**: {profile.work_style}")

    st.divider()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì‚­ì œ", use_container_width=True):
        st.session_state.chat_messages = []
        st.rerun()

# ========================================
# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
# ========================================

for msg in st.session_state.chat_messages:
    role = msg["role"]
    content = msg["content"]
    msg_type = msg.get("type", "normal")

    if role == "user":
        with st.chat_message("user"):
            st.write(content)

    elif role == "assistant":
        with st.chat_message("assistant"):
            st.write(content)

    elif role == "tool":
        # Tool í˜¸ì¶œ ë¡œê·¸
        with st.chat_message("assistant", avatar="ğŸ”§"):
            st.caption(content)

    elif role == "tool_result":
        # Tool ê²°ê³¼ ë¡œê·¸
        with st.chat_message("assistant", avatar="âœ…"):
            with st.expander(f"ğŸ“¦ {msg.get('tool_name', 'Tool')} ê²°ê³¼", expanded=False):
                st.text(content[:500] + "..." if len(content) > 500 else content)

# ========================================
# ì±„íŒ… ì…ë ¥ ì²˜ë¦¬
# ========================================

if prompt := st.chat_input("ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.write(prompt)

    # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state.chat_messages.append({"role": "user", "content": prompt})

    # Agent ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        tool_log_container = st.container()

        full_response = ""

        try:
            # Agent ê°€ì ¸ì˜¤ê¸° (Streamlit ìºì‹±)
            llm = get_gemini() if callable(get_gemini) else get_gemini

            agent = create_agent(
                model=llm,
                tools=tools,
                middleware=[
                    dynamic_system_prompt,  # type:ignore
                    *common_middlewares,
                ],
                checkpointer=InMemorySaver(),
                context_schema=UserProfile,
            )

            chat_messages = [
                {"role": "user" if msg["role"] == "user" else "ai", "content": msg["content"]}
                for msg in st.session_state.chat_messages
            ]
            # í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°: stream_mode="messages"
            # ì°¸ê³ : https://docs.langchain.com/oss/python/langchain/streaming
            for chunk in agent.stream(
                {"messages": st.session_state.chat_messages},
                {"configurable": {"thread_id": "1"}},
                context=profile,
                stream_mode="messages",
            ):
                # chunkëŠ” (message, metadata) íŠœí”Œ í˜•íƒœ
                # íŠœí”Œ ì–¸íŒ¨í‚¹ í™•ì¸
                if not isinstance(chunk, tuple) or len(chunk) != 2:
                    continue

                msg, metadata = chunk

                # ë©”íƒ€ë°ì´í„°ê°€ dictì¸ì§€ í™•ì¸
                if not isinstance(metadata, dict):
                    continue

                # ë©”íƒ€ë°ì´í„°ì—ì„œ í˜„ì¬ ë…¸ë“œ í™•ì¸
                node_name = metadata.get("langgraph_node", "")

                # ë©”ì‹œì§€ í´ë˜ìŠ¤ í™•ì¸
                msg_class = msg.__class__.__name__
                print(msg)

                # Tool ë…¸ë“œì—ì„œì˜ ë©”ì‹œì§€ ì²˜ë¦¬
                if "tools" in node_name.lower():
                    # Tool í˜¸ì¶œ ê°ì§€ (AIMessageChunk with tool_calls)
                    if msg_class == "AIMessageChunk" and hasattr(msg, "tool_calls") and msg.tool_calls:
                        for tool_call in msg.tool_calls:
                            if isinstance(tool_call, dict):
                                tool_name = tool_call.get("name", "Unknown")
                                if tool_name and tool_name != "Unknown":
                                    tool_log = f"ğŸ”§ **{tool_name}** í˜¸ì¶œ ì¤‘..."

                                    with tool_log_container:
                                        with st.chat_message("assistant", avatar="ğŸ”§"):
                                            st.caption(tool_log)

                                    st.session_state.chat_messages.append(
                                        {"role": "tool", "content": tool_log, "tool_name": tool_name}
                                    )

                    # Tool ê²°ê³¼ ê°ì§€
                    elif msg_class == "ToolMessage":
                        tool_name = getattr(msg, "name", "Unknown")
                        tool_result = getattr(msg, "content", "")
                        tool_call_id = getattr(msg, "tool_call_id", "")

                        with tool_log_container:
                            with st.chat_message("assistant", avatar="âœ…"):
                                with st.expander(f"ğŸ“¦ {tool_name} ê²°ê³¼", expanded=False):
                                    result_preview = str(tool_result)[:500]
                                    if len(str(tool_result)) > 500:
                                        result_preview += "..."
                                    st.text(result_preview)

                        st.session_state.chat_messages.append(
                            {"role": "tool", "content": tool_result, "tool_name": tool_name, "tool_call_id": tool_call_id}
                        )

                # LLM ë…¸ë“œì—ì„œì˜ í† í° ìŠ¤íŠ¸ë¦¬ë°
                elif "model" in node_name.lower() or "agent" in node_name.lower():
                    # AIMessageChunkì—ì„œ í† í° ì¶”ì¶œ
                    if msg_class == "AIMessageChunk" and hasattr(msg, "content"):
                        token = getattr(msg, "content", "")
                        if token:
                            # Tool callsê°€ ì—†ëŠ” ê²½ìš°ë§Œ ì‘ë‹µ í† í°ìœ¼ë¡œ ê°„ì£¼
                            has_tool_calls = hasattr(msg, "tool_calls") and msg.tool_calls
                            if not has_tool_calls:
                                full_response += token
                                # ì‹¤ì‹œê°„ í† í° í‘œì‹œ (ì»¤ì„œ íš¨ê³¼)
                                response_placeholder.markdown(full_response + "â–Œ")

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ (ì»¤ì„œ ì œê±°)
            if full_response:
                response_placeholder.markdown(full_response)
                st.session_state.chat_messages.append({"role": "assistant", "content": full_response})
            else:
                error_msg = "âš ï¸ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                response_placeholder.error(error_msg)
                st.session_state.chat_messages.append({"role": "assistant", "content": error_msg})

        except Exception as e:
            error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            response_placeholder.error(error_msg)
            st.session_state.chat_messages.append({"role": "assistant", "content": error_msg})

            # ë””ë²„ê¹…ìš© ìƒì„¸ ì—ëŸ¬
            with st.expander("ğŸ› ìƒì„¸ ì—ëŸ¬ ì •ë³´"):
                st.exception(e)

# ========================================
# Footer
# ========================================

st.divider()
st.caption("ğŸ’¡ Tip: êµ¬ì²´ì ìœ¼ë¡œ ê³ ë¯¼ì„ ì„¤ëª…í•˜ë©´ ë” ì •í™•í•œ ì¡°ì–¸ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
