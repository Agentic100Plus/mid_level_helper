import os
from datetime import datetime

from langchain.agents import create_agent
from langchain.agents.middleware import ModelRequest, dynamic_prompt
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field

from schemas import UserProfile


class WebSearchSchemas(BaseModel):
    title: str = Field(description="ì œëª©")
    body: str = Field(description="ë³¸ë¬¸ ìš”ì•½")
    href: str = Field(description="ì¶œì²˜")


class WebSearchToolResponseSchemas(BaseModel):
    count: int = Field(description="ê²°ê³¼ ìˆ˜")
    articles: list[WebSearchSchemas] = Field(description="ê²°ê³¼ ë°ì´í„°")


# def test_ddgs_search():
#     """
#     ddgs ê²€ìƒ‰
#     """
#     results: list[dict[str, str]] = DDGS().text(
#         query="ì¤‘ë‹ˆì–´ ê³ ë¯¼",
#         region="kr-kr",
#         max_results=10,
#         page=1,
#         backend="auto",
#     )
#     context = [WebSearchSchemas(**data) for data in results]

#     data = WebSearchToolResponseSchemas(count=len(context), articles=context)
#     assert data.count == 10
from typing import Any

from langchain.agents.middleware import AgentMiddleware, AgentState
from langgraph.runtime import Runtime

from tools.web_search import ddgs_search


class LoggingMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"About to call model with {len(state['messages'])} messages")
        return None

    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None


@dynamic_prompt
def user_role_prompt(request: ModelRequest) -> str:
    base_prompt = "ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í† ëŒ€ë¡œ ë§ì¶¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
    profile = request.runtime.context
    career_level = profile.career_level
    #     profile_text = f"""
    # ê²½ë ¥: {profile.career_level} ({profile.years_of_experience}ë…„ì°¨)
    # ì§ë¬´: {profile.job_role}
    # ê¸°ìˆ ìŠ¤íƒ: {", ".join(profile.tech_stack)}
    # íšŒì‚¬ê·œëª¨: {profile.company_size or "ë¯¸ì§€ì •"}
    # ê·¼ë¬´í˜•íƒœ: {profile.work_style or "ë¯¸ì§€ì •"}
    #     """.strip()
    profile_text = profile.to_context_string()

    base_prompt += f"\n\nì‚¬ìš©ìí”„ë¡œí•„ ì •ë³´: {profile_text}"

    if career_level == "ì£¼ë‹ˆì–´":
        base_prompt += "\n\nì£¼ë‹ˆì–´ ì…ì¥ì—ì„œ ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¤„ì§€ ê³ ë¯¼í•´ì•¼í•´."
    elif career_level == "ì¤‘ë‹ˆì–´":
        base_prompt += (
            "\n\nì¤‘ë‹ˆì–´ ì…ì¥ì—ì„œ ì¡°ê¸ˆ ë” í¬ê´„ì ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼í•´. í¬ê´„ì  ì ‘ê·¼ì„ ìœ„í•´ 'ddgs_search' ë„êµ¬ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼í•´."
        )
        base_prompt += " ë„êµ¬ 'ddgs_search' ë¥¼ ì‚¬ìš©í•´ì„œ ë‚˜ì˜¨ ì •ë³´ë„ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ ì¶”ê°€í•´. "
    else:
        base_prompt += "\n\ní”„ë¡œí•„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ì ‘ê·¼í•´ì•¼í•´."

    #     base_prompt += """JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
    # {{
    #     "insights": ["...", "...", "..."],
    #     "recommendations": ["...", "..."]
    # }}"""
    return base_prompt


def test_analyze_proflile_prompt():
    """
    ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    # profile_text = UserProfile(
    #     name="ê¹€ê°œë°œ",
    #     career_level="ì£¼ë‹ˆì–´",
    #     years_of_experience=3,
    #     job_role="ë°±ì—”ë“œ",
    #     tech_stack=["python", "django", "fastAPI", "AWS"],
    #     company_size="ìŠ¤íƒ€íŠ¸ì—…",
    #     work_style="ì¬íƒ",
    #     created_at=datetime.now(),
    # ).to_context_string()

    # query = "ë…„ì°¨ì— ë¹„í•´ í•  ì¤„ ì•„ëŠ”ê²Œ ë„ˆë¬´ ì—†ë‹¤ê³  ìƒê°í•´"

    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        api_key=os.getenv("GOOGLE_API_KEY"),
        temperature=0.7,  # 0.1 â†’ 0.7 ë³€ê²½ (Tool í˜¸ì¶œ ì¦ê°€)
        max_tokens=1000,  # 500 â†’ 1000 ë³€ê²½ (ë” í’ë¶€í•œ ë‹µë³€)
        timeout=30,
        max_retries=3,
    )

    agent = create_agent(
        model=model,
        tools=[ddgs_search],
        middleware=[
            # SummarizationMiddleware(
            #     model=model,
            #     max_tokens_before_summary=3000,
            #     messages_to_keep=2,
            # ),
            # LoggingMiddleware(),
            user_role_prompt,
        ],
        context_schema=UserProfile,
    )
    result = agent.invoke(
        {
            "messages": [
                {"role": "user", "content": "íŒŒì´ì¬ ë°±ì—”ë“œ ì¤‘ë‹ˆì–´ë¡œì¨ ë­˜í•´ì•¼í• ì§€ ëª¨ë¥´ê² ì–´?"},
            ]
        },
        context=UserProfile(
            name="ê¹€ê°œë°œ",
            career_level="ì¤‘ë‹ˆì–´",
            years_of_experience=4,
            job_role="ë°±ì—”ë“œ",
            tech_stack=["python", "django", "fastAPI", "AWS"],
            company_size="ìŠ¤íƒ€íŠ¸ì—…",
            work_style="ì¬íƒ",
            created_at=datetime.now(),
        ),
    )

    # Tool í˜¸ì¶œ í™•ì¸
    print("\n" + "=" * 80)
    print("ğŸ“‹ Result Messages:")
    print("=" * 80)

    for msg in result["messages"]:
        msg_type = msg.__class__.__name__
        print(f"\nğŸ”¹ Message Type: {msg_type}")

        # Tool í˜¸ì¶œ í™•ì¸
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            print("  âœ… Tool Calls Found!")
            for tool_call in msg.tool_calls:
                print(f"     ğŸ”§ Tool: {tool_call['name']}")
                print(f"     ğŸ“ Args: {tool_call['args']}")

        # Tool ê²°ê³¼ í™•ì¸
        if msg_type == "ToolMessage":
            print(f"  ğŸ“¦ Tool Result: {msg.content[:200]}...")

        # AI ìµœì¢… ì‘ë‹µ
        if msg_type == "AIMessage" and hasattr(msg, "content") and msg.content:
            print(f"  ğŸ’¬ AI Response: {msg.content[:200]}...")

    assert 1 == 2
