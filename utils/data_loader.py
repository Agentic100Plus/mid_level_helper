"""ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°."""

import re
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd


def load_csv_data(csv_path: str = "data/mid_level_data_unique_3000.csv") -> pd.DataFrame:
    """CSV íŒŒì¼ ë¡œë“œ.

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        DataFrame with columns: ê¸€ ì œëª©, ì¶œì²˜, í•µì‹¬ í‚¤ì›Œë“œ, ë¬¸ì œì  ìš”ì•½, ê¸€ ë‚´ìš© ìš”ì•½
    """
    path = Path(csv_path)
    if not path.exists():
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    df = pd.read_csv(csv_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
    return df


def extract_category(problem_summary: str) -> str:
    """ë¬¸ì œì  ìš”ì•½ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ.

    Examples:
        "ì¬íƒê·¼ë¬´ë¡œ ì¸í•´... (ì„±ì¥í†µ ì´ìŠˆ ì‚¬ë¡€ 1)" -> "ì„±ì¥í†µ"
        "... (growth challenge ì´ìŠˆ ì‚¬ë¡€ 802)" -> "growth challenge"

    Args:
        problem_summary: ë¬¸ì œì  ìš”ì•½ í…ìŠ¤íŠ¸

    Returns:
        ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ ë˜ëŠ” "ê¸°íƒ€"
    """
    # í•œêµ­ì–´ ì¹´í…Œê³ ë¦¬ íŒ¨í„´
    korean_patterns = [
        r'\((\S+)\s+ì´ìŠˆ\s+ì‚¬ë¡€',  # (ì„±ì¥í†µ ì´ìŠˆ ì‚¬ë¡€ 1)
        r'\((\S+)\s+ê´€ë ¨\s+ìƒí™©',  # (ì„±ì¥í†µ ê´€ë ¨ ìƒí™©)
    ]

    for pattern in korean_patterns:
        match = re.search(pattern, problem_summary)
        if match:
            return match.group(1)

    # ì˜ì–´ ì¹´í…Œê³ ë¦¬ íŒ¨í„´
    english_pattern = r'\((\w+(?:\s+\w+)?)\s+ì´ìŠˆ\s+ì‚¬ë¡€'
    match = re.search(english_pattern, problem_summary)
    if match:
        return match.group(1)

    return "ê¸°íƒ€"


def extract_keywords_list(keywords: str) -> List[str]:
    """í‚¤ì›Œë“œ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.

    Args:
        keywords: "í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3" í˜•ì‹

    Returns:
        ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    """
    if pd.isna(keywords) or not keywords:
        return []
    return [k.strip() for k in keywords.split(",") if k.strip()]


def combine_text_for_embedding(row: pd.Series) -> str:
    """ë ˆì½”ë“œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œë¥¼ ê²°í•©í•˜ì—¬ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±.

    Args:
        row: DataFrameì˜ í•œ í–‰

    Returns:
        ê²°í•©ëœ í…ìŠ¤íŠ¸ (ì œëª© + í‚¤ì›Œë“œ + ë¬¸ì œì  ìš”ì•½ + ë‚´ìš© ìš”ì•½)
    """
    parts = []

    if pd.notna(row.get("ê¸€ ì œëª©")):
        parts.append(f"ì œëª©: {row['ê¸€ ì œëª©']}")

    if pd.notna(row.get("í•µì‹¬ í‚¤ì›Œë“œ")):
        parts.append(f"í‚¤ì›Œë“œ: {row['í•µì‹¬ í‚¤ì›Œë“œ']}")

    if pd.notna(row.get("ë¬¸ì œì  ìš”ì•½")):
        parts.append(f"ë¬¸ì œ: {row['ë¬¸ì œì  ìš”ì•½']}")

    if pd.notna(row.get("ê¸€ ë‚´ìš© ìš”ì•½")):
        parts.append(f"ë‚´ìš©: {row['ê¸€ ë‚´ìš© ìš”ì•½']}")

    return "\n".join(parts)


def create_metadata(row: pd.Series, index: int) -> Dict[str, str]:
    """ë ˆì½”ë“œì—ì„œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±.

    Args:
        row: DataFrameì˜ í•œ í–‰
        index: ë ˆì½”ë“œ ì¸ë±ìŠ¤

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    category = extract_category(row.get("ë¬¸ì œì  ìš”ì•½", ""))

    return {
        "id": str(index),
        "title": str(row.get("ê¸€ ì œëª©", "")),
        "source": str(row.get("ì¶œì²˜", "")),
        "keywords": str(row.get("í•µì‹¬ í‚¤ì›Œë“œ", "")),
        "problem_summary": str(row.get("ë¬¸ì œì  ìš”ì•½", "")),
        "category": category,
    }


def prepare_documents_for_vectorstore(
    df: pd.DataFrame,
) -> Tuple[List[str], List[Dict[str, str]]]:
    """ë²¡í„° ìŠ¤í† ì–´ìš© ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„.

    Args:
        df: ì›ë³¸ DataFrame

    Returns:
        (texts, metadatas) íŠœí”Œ
        - texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        - metadatas: ê° í…ìŠ¤íŠ¸ì— ëŒ€ì‘í•˜ëŠ” ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    texts = []
    metadatas = []

    for idx, row in df.iterrows():
        text = combine_text_for_embedding(row)
        metadata = create_metadata(row, idx)

        texts.append(text)
        metadatas.append(metadata)

    print(f"âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ: {len(texts)}ê°œ")
    return texts, metadatas


def get_category_distribution(df: pd.DataFrame) -> Dict[str, int]:
    """ì¹´í…Œê³ ë¦¬ë³„ ë ˆì½”ë“œ ìˆ˜ ì§‘ê³„.

    Args:
        df: ì›ë³¸ DataFrame

    Returns:
        {ì¹´í…Œê³ ë¦¬: ê°œìˆ˜} ë”•ì…”ë„ˆë¦¬
    """
    categories = df["ë¬¸ì œì  ìš”ì•½"].apply(extract_category)
    return categories.value_counts().to_dict()


def print_data_stats(df: pd.DataFrame) -> None:
    """ë°ì´í„° í†µê³„ ì¶œë ¥."""
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°ì´í„° í†µê³„")
    print("=" * 60)
    print(f"ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
    print(f"ì»¬ëŸ¼: {', '.join(df.columns)}")

    print("\nì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    category_dist = get_category_distribution(df)
    for category, count in sorted(category_dist.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {category}: {count}ê°œ")

    print("\nìƒ˜í”Œ ë ˆì½”ë“œ:")
    sample = df.iloc[0]
    print(f"  ì œëª©: {sample['ê¸€ ì œëª©']}")
    print(f"  í‚¤ì›Œë“œ: {sample['í•µì‹¬ í‚¤ì›Œë“œ']}")
    print(f"  ì¹´í…Œê³ ë¦¬: {extract_category(sample['ë¬¸ì œì  ìš”ì•½'])}")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    df = load_csv_data()
    print_data_stats(df)

    # ë¬¸ì„œ ì¤€ë¹„ í…ŒìŠ¤íŠ¸
    texts, metadatas = prepare_documents_for_vectorstore(df)

    print("\nìƒ˜í”Œ ì„ë² ë”© í…ìŠ¤íŠ¸:")
    print(texts[0][:300] + "...")

    print("\nìƒ˜í”Œ ë©”íƒ€ë°ì´í„°:")
    print(metadatas[0])
