import os
import sys

# ===== í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€ =====
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)
# =====================================

from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec
from tqdm import tqdm

from utils.data_loader import load_csv_data, prepare_documents_for_vectorstore

load_dotenv()

# ============================================
# 1. Pinecone ì´ˆê¸°í™”
# ============================================
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

index_name = os.getenv("PINECONE_INDEX_NAME", "mid-level-helper")

if index_name not in pc.list_indexes().names():
    print(f"ğŸ“¦ ì¸ë±ìŠ¤ ìƒì„± ì¤‘: {index_name}")
    pc.create_index(
        name=index_name,
        dimension=4096,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )
    print("âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
else:
    print(f"âœ… ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸: {index_name}")


# Pinecone ì¸ë±ìŠ¤ ë¡œë“œ
index = pc.Index(index_name)

# ============================================
# 2. Upstage ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (OpenAI Wrapper)
# ============================================
client = OpenAI(api_key=os.getenv("UPSTAGE_API_KEY"), base_url="https://api.upstage.ai/v1/solar")

print("\n" + "=" * 60)
print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
print("=" * 60)

# ë°ì´í„° ë¡œë“œ
df = load_csv_data()
texts, metadatas = prepare_documents_for_vectorstore(df)
print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")

# Pinecone ë°°ì¹˜ ì‚¬ì´ì¦ˆ
BATCH_SIZE = 100


def create_embeddings_batch(texts: list[str]) -> list[list[float]]:
    """í…ìŠ¤íŠ¸ ë°°ì¹˜ -> ì„ë² ë”© ë³€í™˜"""
    try:
        res = client.embeddings.create(input=texts, model="embedding-query")
        return [emb.embedding for emb in res.data]
    except Exception as e:
        print(f"â˜ ï¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        raise


def pinecone_batch(
    ids: list[str],
    embeddings: list[list[float]],
    metadatas: list[dict],
) -> list[dict]:
    """Pinecone ì—…ë¡œë“œ ë°ì´í„° í¬ë§·"""
    return [
        {"id": id_, "values": embedding, "metadata": metadata} for id_, embedding, metadata in zip(ids, embeddings, metadatas)
    ]


print("\n" + "=" * 60)
print("ğŸ”„ ì„ë² ë”© ìƒì„± ë° ì—…ë¡œë“œ ì¤‘...")
print("=" * 60)

total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE
uploaded_count = 0

for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="ë°°ì¹˜ ì²˜ë¦¬"):
    batch_texts = texts[i : i + BATCH_SIZE]
    batch_metadatas = metadatas[i : i + BATCH_SIZE]
    batch_ids = [metadata["id"] for metadata in batch_metadatas]

    # ì„ë² ë”© ìƒì„±
    embeddings = create_embeddings_batch(batch_texts)

    # Pinecone í¬ë§· ë³€í™˜
    vectors = pinecone_batch(batch_ids, embeddings, batch_metadatas)

    # Pineconeì— ì—…ë¡œë“œ
    try:
        index.upsert(vectors=vectors, namespace="20251029_crawling")
        uploaded_count += len(vectors)
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ (ë°°ì¹˜ {i // BATCH_SIZE + 1}): {e}")
        raise

print(f"\nâœ… ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}ê°œ ë²¡í„°")

# ============================================
# 5. ê²€ì¦
# ============================================
print("\n" + "=" * 60)
print("ğŸ” ê²€ì¦ ì¤‘...")
print("=" * 60)

stats = index.describe_index_stats()
print(f"ì´ ë²¡í„° ìˆ˜: {stats.total_vector_count}")
print(f"ì°¨ì›: {stats.dimension}")

# ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
print("\nìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
test_query = "ì¬íƒê·¼ë¬´í•˜ë©´ì„œ ë™ê¸°ë¶€ì—¬ê°€ ë–¨ì–´ì ¸ìš”"
test_embedding = create_embeddings_batch([test_query])[0]

results = index.query(vector=test_embedding, top_k=3, include_metadata=True, namespace="20251029_crawling")

for i, match in enumerate(results.matches, 1):
    print(f"\n[{i}] ìœ ì‚¬ë„: {match.score:.4f}")
    print(f"ì œëª©: {match.metadata.get('title', 'N/A')}")
    print(f"ì¹´í…Œê³ ë¦¬: {match.metadata.get('category', 'N/A')}")

print("\n" + "=" * 60)
print("âœ… ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ!")
print("=" * 60)
